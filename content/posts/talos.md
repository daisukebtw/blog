---
title: Talos Linux Kubernetes cluster (3 node)
date: 2025-02-25
author: "bezdar"
categories:
  - "linux"
  - "kubernetes"
  - "ansible"
cover:
  image: https://cdn.prod.website-files.com/6527fe8ad7301efb15574cc7/65535c0459f914d06ddeef7a_Kubernetes-Everything-You-Need-to-Know-2048x1159.png
---

# Introduction

⚠️ WARNING ⚠️

- I am NOT **Kubernetes** expert
-   This is solely for my own learning. If you get something useful out of my ramblings, that’s great, but it’s not my primary goal.

## Why Kubernetes?

Because I am about to reach my goal of being a **DevOps Engineer**, and that role requires me to know **Kubernetes** at Advanced Level and there is no better way to learn it if not on practice that's why **Kubernetes**

## Why Talos?

I had multiple options to choose from

- **K3s**
- **Kubeadm**
- **Talos Linux**

I have chosen **Talos Linux** because it is lightweight, secure and is production grade OS.

# Install Talos Linux cluster

- My setup is 3 VMs across 3 physical hosts
- VMs are in dedicated VLAN which is 10.10.30.0/24

For Installation I have wrote **Ansible Playbook** to automate **Talos Linux** cluster deployment, I have uploaded it to my **[GitHub repository](https://github.com/daisukebtw/playbooks/tree/main/talos)**

```
git clone https://github.com/daisukebtw/playbooks.git
cd playbooks/talos

# Edit inventory/hosts.ini and inventory/group_vars/all.yml
# Also edit roles/generate-config/files/patch.yml with your own configuration
   
# Then run init.yml playbook using command below
ansible-playbook -i path/to/inventory init.yml
```

**Ansible Playbook** will generate configuration files to init **Talos Linux** cluster, apply configuration files to specific hosts configured in group_vars/all.yml, install **Cilium** as main **CNI (Container Network Interface)**, install **MetalLB** for automatic **VIP (Virtual IP)** configuration, **Metrics Server** to track cluster resource usage (CPU, RAM usage)

# Setup Talos Linux cluster

From the beginning plan was to create a cluster to learn on, and that is why I planned to set up next Stack.

- **Grafana + Prometheus + Loki** -- Logging, Monitoring Stack
- **ArgoCD** -- Automatic Deploy to **Kubernetes** cluster from **VCS (Version Control System)**
- **Dynamic DNS** -- To Automatically track and change Public IP on **Cloudflare**
- **Traefik + Cert-manager** -- Reverse Proxy
- **Own Projects** -- [IPTY (Check Your Public IP)](https://github.com/daisukebtw/ipty), [Blog (Using HUGO)](https://github.com/daisukebtw/blog) etc.

## Grafana + Prometheus + Loki

### Grafana


```
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
helm install grafana grafana/grafana --namespace monitoring --create-namespace
```

Also created **Traefik IngressRoute** to make service secure using certificates generated by **Cert-manager**

### Loki

```
helm install loki-stack grafana/loki-stack --values path/to/values.yaml --namespace monitoring
```

for **Loki** I have used next values

```
loki:
  enabled: true
  isDefault: true
  url: http://{{(include "loki.serviceName" .)}}:{{ .Values.loki.service.port }}
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  datasource:
    jsonData: "{}"
    uid: ""
  persistence:
    enabled: true
    storageClassName: nfs-csi
    accessModes: ["ReadWriteOnce"]
    size: 10Gi

promtail:
  enabled: true
  config:
     logLevel: info
     serverPort: 3101
     clients:
       - url: http://{{ .Release.Name }}:3100/loki/api/v1/push
```

### Prometheus

```
helm install prometheus prometheus-community/prometheus --namespace monitoring
```

Added **Prometheus** and **Loki** with its cluster names and port

- http://loki-stack:3100
- http://prometheus-server

## ArgoCD

Installed **ArgoCD** HA using its [documentation](https://argo-cd.readthedocs.io/en/stable/operator-manual/installation/)

```
kubectl create namespace argocd
kubectl apply -f https://github.com/argoproj/argo-cd/blob/master/manifests/ha/install.yaml --namespace argocd
```

I used **ArgoCD CLI** to get password

```
argocd admin initial-password -n argocd
```

Also created **Traefik IngressRoute** to make service secure using certificates generated by **Cert-manager**

## Dynamic DNS

I have moved **Dynamic DNS** container from **Docker** to **Kubernetes** by creating **Kubernetes Manifest** for that

## Traefik + Cert-manager

I have deployed **Traefik** and **Cert-manager** using **Helm**

### Traefik

```
helm repo add traefik https://traefik.github.io/charts
helm repo update
helm install traefik traefik/traefik \
  --values path/to/values.yaml \
  --namespace traefik \
  --create-namespace
```

for **Traefik** I have used next values

```
port:
  web:
    redirectTo:
      port: websecure
deployment:
  replicas: 2
service:
  spec:
    loadBalancerIP: 10.10.30.70 # MetalLB VIP
```

### Cert-manager

```
helm repo add jetstack https://charts.jetstack.io --force-update
helm repo update
helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.17.0 \
  --set crds.enabled=true
```
 
Also created **ClusterIssuer**, **Certificates** by **Cert-manager** [documentation](https://cert-manager.io/docs/)

